{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phd-Master/LSTM_RNN_Tutorials_with_Demo/blob/master/Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3urR99D6z0ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# must have the shape of W3 which is (output_dim_size, hidden_state_size)\n",
        "dW3 = np.dot(e.T, h)\n",
        "dh = np.dot(e, W3)   \n",
        "# dbo =e.1, since we have batch we use np.sum \n",
        "# e is a vector, when it is subtracted from label, the result will be added to dbo\n",
        "dbo = np.sum(e, axis=0) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFsvEvNM1IRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# must have the shape of W3 which is (output_dim_size, hidden_state_size)\n",
        "dW3 = np.dot(e.T, h)\n",
        "# when calculating the dh, we also add the dh from the next timestep as well\n",
        "# when we are in the last timestep, the h_gradient_or_dh is initially zero.\n",
        "dh = np.dot(e, W3) + h_gradient_or_dh  \n",
        "# dbo =e.1, since we have batch we use np.sum \n",
        "# e is a vector, when it is subtracted from label, the result will be added to dbo\n",
        "dbo = np.sum(e, axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkRVHNkF1N_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    # the input part\n",
        "    dtanh = (1 - h * h) * dh\n",
        " \n",
        "    # compute the gradient of the loss with respect to W1\n",
        "\t# this is actually not needed!, because its not a tunable parameter!\n",
        "    # dxt = np.dot(dtanh, W1.T) \n",
        "    # must have the shape of (input_dim_size, hidden_state_size)\n",
        "    dW1 = np.dot(xt.T, dtanh)\n",
        " \n",
        "    # compute the gradient with respect to W2\n",
        "    dh_prev = np.dot(dtanh, W2.T)\n",
        "    # shape must be (HiddenSize, HiddenSize)\n",
        "    dW2 = np.dot(h_prev.T, dtanh)\n",
        " \n",
        "    # dbh += dtanh.1, we use sum, since we have a batch\n",
        "    dbh = np.sum(dtanh, axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b-gnIfY1Wt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rnn_cell_backward(xt,\n",
        "                        h,\n",
        "                        h_prev,\n",
        "                        output,\n",
        "                        true_label,\n",
        "                        h_gradient_or_dh,\n",
        "                        W1,\n",
        "                        W2,\n",
        "                        W3 ):\n",
        "    \"\"\"\n",
        "       xt:   \n",
        "          is the input in the form of (batch_size, input_dim_size)\n",
        "       h :   \n",
        "          is the next hidden state\n",
        "       h_prev: \n",
        "          is the previous hidden state \n",
        "       output: \n",
        "          is the output of the rnn cell!\n",
        "       true_label: \n",
        "          is the true label for the current output\n",
        "    h_gradient_or_dh: \n",
        "\t\t\tThe dh which in the beginning is zero\n",
        "\t\t\tand is updated as we go backward in the backprogagation.\n",
        "\t\t\tthe dh for the next round, would come from the dh_prev.\n",
        "\t\t\tremember the backward pass is essentially a loop! and we\n",
        "\t\t\tstart at the end and traverse back to the beginning!\n",
        "    \"\"\"\n",
        "\n",
        "    e = np.copy(output)\n",
        "    \n",
        "\t# This is error_t = output_t - label_t\n",
        "\te[np.arange(e.shape[0]), np.argmax(true_label, axis=1)] -=1\n",
        "    # This is used for our loss to see how well we are doing during training.\n",
        "\tper_ts_loss = output[np.arange(output.shape[0]), np.argmax(true_label, axis=1)].sum()\n",
        "\t\n",
        "    # this is the non-vectorized version of the above snippet\n",
        "\t# per_ts_loss = 0\n",
        "    # for i, idx in enumerate(np.argmax(true_label, axis=1)):\n",
        "        # e[i, idx] -=1\n",
        "        # per_ts_loss += output[i, idx]    \n",
        "\n",
        "\tdW3 = np.dot(e.T, h)\n",
        "\t# when calculating the dh, we also add the dh from the next timestep as well\n",
        "\t# when we are in the last timestep, the h_gradient_or_dh is initially zero.\n",
        "\tdh = np.dot(e, W3) + h_gradient_or_dh  \n",
        "\t# dbo = e.1, since we have batch we use np.sum \n",
        "\t# e is a vector, when it is subtracted from label, the result will be added to dbo\n",
        "\tdbo = np.sum(e, axis=0)\n",
        "\n",
        "\t\n",
        "    # the input part\n",
        "    dtanh = (1 - h * h) * dh\n",
        " \n",
        "    # compute the gradient of the loss with respect to W1\n",
        "\t# this is actually not needed! bcause Xt is not a tunable\n",
        "\t# parameter, thererfore we dont need it gradient. \n",
        "    # dxt = np.dot(dtanh, W1.T) \n",
        "\t\n",
        "    # must have the shape of (input_dim_size, hidden_state_size)\t\n",
        "    dW1 = np.dot(xt.T, dtanh)\n",
        " \n",
        "    # compute the gradient with respect to W2\n",
        "    dh_prev = np.dot(dtanh, W2.T)\n",
        "    # shape must be (HiddenSize, HiddenSize)\n",
        "    dW2 = np.dot(h_prev.T, dtanh)\n",
        " \n",
        "    # dbh += dtanh.1, we use sum, since we have a batch\n",
        "    dbh = np.sum(dtanh, axis=0)\n",
        "\n",
        "    return dW1, dW2, dW3, dbh, dbo, dh_prev, per_ts_loss\n",
        "\n",
        "\n",
        "def rnn_layer_backward( Xt,\n",
        "                        labels,\n",
        "                        H,\n",
        "                        O,\n",
        "                        W1,\n",
        "                        W2,\n",
        "                        W3,\n",
        "                        bh,\n",
        "                        bo ):\n",
        "\t\"\"\"\n",
        "\t   Xt: \t\t\t\n",
        "\t\t\tThe input in the form of (batch_size, time-step, input_dim_size)\n",
        "\t   labels:\t\t\n",
        "\t\t\tThe labels for each output with the shape (batch_size, time-step, outputsize)\n",
        "       H :      \t\n",
        "\t\t\tAll the hidden states from the forward pass with the shape (batch_size, time-step, hidden_state_size)\n",
        "       O:       \t\n",
        "\t\t\tAll the outputs from the forward pass \n",
        "\n",
        "\t   W1,W2,W3,bh and bo: \n",
        "\t\t\tAre network parameters.\n",
        "\t\n",
        "\t\"\"\"\n",
        "\n",
        "    dW1 = np.zeros_like(W1)\n",
        "    dW2 = np.zeros_like(W2)\n",
        "    dW3 = np.zeros_like(W3)\n",
        "    # must have the shape(batch, hiddensize)\n",
        "    dbh = np.zeros_like(bh)\n",
        "    # must have the shape(batch, outputsize)\n",
        "    dbo = np.zeros_like(bo)\n",
        "    dh = np.zeros_like(H[:, 0, :])\n",
        "\n",
        "    _, T_x , _= Xt.shape\n",
        "    loss = 0\n",
        "\n",
        "    for t in reversed(range(T_x)):\n",
        "        dw1, dw2, dw3, dbh, dbo, dh_prev, per_ts_loss = rnn_cell_backward(Xt[:, t, :],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t H[:, t, :],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t H[:, t - 1, :],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t O[:, t, :],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t labels[:, t, :],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t dh,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t W1, W2,W3 )\n",
        "        dh = dh_prev\n",
        "\n",
        "        dW1 += dw1\n",
        "        dW2 += dw2\n",
        "        dW3 += dw3\n",
        "        dbh += dbh\n",
        "        dbo += dbo\n",
        "        \n",
        "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
        "        loss -= np.log(per_ts_loss)\n",
        "\n",
        "    return dW1, dW2, dW3, dbh, dbo, dh, loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R58fdmaC1aPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "input_dim_size = 3\n",
        "hidden_state_size = 5\n",
        "outputsize = 3\n",
        "batch = 2\n",
        "timesteps = 4\n",
        "\n",
        "W1 = np.random.randn(input_dim_size, hidden_state_size)\n",
        "W2 = np.random.randn(hidden_state_size, hidden_state_size)\n",
        "W3 = np.random.randn(outputsize, hidden_state_size)\n",
        "bh = np.random.randn(hidden_state_size)\n",
        "bo = np.random.randn(outputsize)\n",
        "\n",
        "Xt = np.random.rand(batch, timesteps, input_dim_size)\n",
        "# label has the same shape as input, because we are creating outputs just like the inputs. \n",
        "Labels = np.random.rand(batch, timesteps, input_dim_size)\n",
        "h_previous = np.zeros(shape=(batch, hidden_state_size))\n",
        "\n",
        "Outputs, HiddenStates = rnn_layer_forward(Xt, hidden_state_size, W1, W2, W3,bh, bo)\n",
        "dW1, dW2, dW3, dbh, dbo, dh, loss = rnn_layer_backward(Xt, Labels, HiddenStates, Outputs,W1, W2, W3,bh, bo )\n",
        "print(dW1.shape)\n",
        "print(\"dh_prev[0] =\", dh[0])\n",
        "print(\"dh.shape =\", dh.shape)\n",
        "print(\"dW1[0] =\", dW1[0])\n",
        "print(\"dW1.shape =\", dW1.shape)\n",
        "print(\"dW2[0] =\", dW2[0])\n",
        "print(\"dW2.shape =\", dW2.shape)\n",
        "print(\"dbh[0] =\", dbh[0])\n",
        "print(\"dbh.shape =\", dbh.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LAL-J8U1eqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# in the name of Allah\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.utils.extmath import softmax\n",
        "\n",
        "class RNNClass(object):\n",
        "    def __init__(self, input_dim_size, outputsize, hidden_state_size=100, mode=1, clipping_threshold=0.5):\n",
        "\n",
        "        np.random.seed(1)\n",
        "\n",
        "        # weights and biases - primitive method!\n",
        "        self.W1 = np.random.randn(input_dim_size, hidden_state_size) * 0.01\n",
        "        self.W2 = np.random.randn(hidden_state_size, hidden_state_size) * 0.01\n",
        "        self.W3 = np.random.randn(outputsize, hidden_state_size) * 0.01\n",
        "\n",
        "        # to fight vanishing gradient to some extend, we need proper weight initialization\n",
        "        # Xavier-Glorot initialization\n",
        "        # self.W1 = np.random.uniform( -np.sqrt(1./input_dim_size), np.sqrt(1./input_dim_size),(input_dim_size, hidden_state_size))\n",
        "        # self.W2 = np.random.uniform( -np.sqrt(1./hidden_state_size), np.sqrt(1./hidden_state_size),(hidden_state_size, hidden_state_size) )\n",
        "        # self.W3 = np.random.uniform( -np.sqrt(1./hidden_state_size), np.sqrt(1./hidden_state_size),(outputsize, hidden_state_size) )\n",
        "\n",
        "        # This can be zero, it can also be randomly initialized\n",
        "        self.bh = np.ones(shape=(hidden_state_size))\n",
        "        self.bo = np.ones(shape=(outputsize))\n",
        "\n",
        "        self.input_dim_size = input_dim_size\n",
        "        self.outputsize = outputsize\n",
        "        self.hidden_state_size = hidden_state_size\n",
        "\n",
        "        self.mode = mode\n",
        "        self.clipping_threshold = clipping_threshold\n",
        "\n",
        "        print('W1: ', self.W1.shape)\n",
        "        print('W2: ', self.W2.shape)\n",
        "        print('W3: ', self.W3.shape)\n",
        "        print('bh: ', self.bh.shape)\n",
        "        print('bo: ', self.bo.shape)\n",
        "\n",
        "    def rnn_cell_foward(self, xt, h0):\n",
        "        \"\"\"\n",
        "            Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
        "            activation function.\n",
        "            The input data has dimension D(vocabsize in case we have nlp use), the hidden state\n",
        "            has dimension H, and we use a minibatch size of N.\n",
        "            Inputs:\n",
        "            - x: Input data for this timestep, of shape (Batch_size, vocabsize_or_basically_input_dim_size).\n",
        "            - h0: Hidden state from previous timestep, of shape (Batch_size, HiddenSize)\n",
        "\n",
        "            Returns :\n",
        "            - h_t: Next hidden state, of shape (Batch_size, HiddenSize)\n",
        "            - output: output , of shape (Batch_size, outputsize)\n",
        "       \"\"\"\n",
        "\n",
        "        h_t = np.tanh(np.dot(xt, self.W1) + np.dot(h0, self.W2) + self.bh)\n",
        "        o = np.dot(h_t, self.W3.T) + self.bo\n",
        "        o_t = softmax(o)\n",
        "\n",
        "        return o_t, h_t\n",
        "\n",
        "    def rnn_layer_forward(self, Xt, H=None):\n",
        "        \"\"\"\n",
        "            Runs a forward pass on the input data Xt and optional hiddenstate H.\n",
        "            if H is initialized, the initial hiddenstate (h0) will be initialized\n",
        "            from the last hiddenstate using this optional argument.\n",
        "            Inputs:\n",
        "            - Xt: The input data of shape (Batch_size, time_steps, input_dim_size)\n",
        "            - H(Optional): an array containing hiddenstates from previous example\n",
        "              of shape(Batch_size, timesteps, HiddenStateSize)\n",
        "\n",
        "            Returns :\n",
        "            - O: The output for the current layer of shape (Batch_size, timesteps, outputsize)\n",
        "            - H: The hiddenstates for the current layer of shape (Batch_size, timesteps, HiddenStateSize)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        batch, T_x, input_dim_size = Xt.shape\n",
        "        H = np.zeros(shape=(batch, T_x, self.hidden_state_size))\n",
        "        O = np.zeros(shape=(batch, T_x, self.outputsize))\n",
        "\n",
        "        if (self.mode == 1):\n",
        "            h_previous = np.zeros(shape=(batch, self.hidden_state_size))\n",
        "        else:\n",
        "            h_previous = H[:, -1, :]\n",
        "\n",
        "        for t in range(T_x):\n",
        "            output, h_t = self.rnn_cell_foward(Xt[:, t, :], h_previous)\n",
        "            H[:, t, :] = h_t\n",
        "            O[:, t, :] = output\n",
        "\n",
        "            # Our current/new hiddenstate will be the previous hiddenstate for the next round,\n",
        "            h_previous = h_t\n",
        "\n",
        "        return O, H\n",
        "\n",
        "    def rnn_cell_backward(self, xt, h, h_prev, output, true_label, dh_next):\n",
        "        \"\"\"\n",
        "            Runs a single backward pass once.\n",
        "            Inputs:\n",
        "            - xt: The input data of shape (Batch_size, input_dim_size)\n",
        "            - h:  The next hidden state at timestep t(which come from the forward pass)\n",
        "            - h_prev: The previous hidden state at timestep t-1\n",
        "            - output : The output at the current timestep\n",
        "            - true_label: The label for the current timestep, used for calcuating loss\n",
        "            - dh_next: The gradient of hiddent state h (dh) which in the beginning\n",
        "                is zero and is updated as we go backward in the backprogagation.\n",
        "                the dh for the next round, would come from the 'dh_prev' as we will see shortly!\n",
        "                Just remember the backward pass is essentially a loop! and we start at the end \n",
        "                and traverse back to the beginning!\n",
        "\n",
        "            Returns : \n",
        "            - dW1 : The gradient for W1\n",
        "            - dW2 : The gradient for W2\n",
        "            - dW3 : The gradient for W3\n",
        "            - dbh : The gradient for bh\n",
        "            - dbo : The gradient for bo\n",
        "            - dh_prev : The gradient for previous hiddenstate at timestep t-1. this will be used\n",
        "            as the next dh for the next round of backpropagation.\n",
        "            - per_ts_loss  : The loss for current timestep.\n",
        "        \"\"\"\n",
        "        e = np.copy(output)\n",
        "        # correct idx for each row(sample)!\n",
        "        idxs = np.argmax(true_label, axis=1)\n",
        "        # number of rows(samples) in our batch\n",
        "        rows = np.arange(e.shape[0])\n",
        "        # This is the vectorized version of error_t = output_t - label_t or simply e = output[t] - 1\n",
        "        # where t refers to the index in which label is 1. \n",
        "        e[rows, idxs] -= 1\n",
        "        # This is used for our loss to see how well we are doing during training.\n",
        "        per_ts_loss = output[rows, idxs].sum()\n",
        "\n",
        "        # must have shape of W3 which is (vocabsize_or_output_dim_size, hidden_state_size)\n",
        "        dW3 = np.dot(e.T, h)\n",
        "        # dbo = e.1, since we have batch we use np.sum\n",
        "        # e is a vector, when it is subtracted from label, the result will be added to dbo\n",
        "        dbo = np.sum(e, axis=0)\n",
        "        # when calculating the dh, we also add the dh from the next timestep as well\n",
        "        # when we are in the last timestep, the dh_next is initially zero.\n",
        "        dh = np.dot(e,  self.W3) + dh_next  # from later cell\n",
        "        # the input part\n",
        "        dtanh = (1 - h * h) * dh\n",
        "        # dbh = dtanh.1, we use sum, since we have a batch\n",
        "        dbh = np.sum(dtanh, axis=0)\n",
        "\n",
        "        # compute the gradient of the loss with respect to W1\n",
        "        # this is actually not needed! we only care about tunable\n",
        "        # parameters, so we are only after, W1,W2,W3, db and do\n",
        "        # dxt = np.dot(dtanh, W1.T)\n",
        "\n",
        "        # must have the shape of (vocab_size, hidden_state_size)\n",
        "        dW1 = np.dot(xt.T, dtanh)\n",
        "        # shape must be (HiddenSize, HiddenSize)\n",
        "        dW2 = np.dot(h_prev.T, dtanh)\n",
        "        # compute the gradient with respect to W2\n",
        "        dh_prev = np.dot(dtanh, self.W2.T)\n",
        "\n",
        "\n",
        "        return dW1, dW2, dW3, dbh, dbo, dh_prev, per_ts_loss\n",
        "\n",
        "    def rnn_layer_backward(self, Xt, labels, H, O):\n",
        "        \"\"\"\n",
        "            Runs a full backward pass on the given data. and returns the gradients.\n",
        "            Inputs: \n",
        "            - Xt: The input data of shape (Batch_size, timesteps, input_dim_size)\n",
        "            - labels: The labels for the input data\n",
        "            - H: The hiddenstates for the current layer prodced in the foward pass \n",
        "              of shape (Batch_size, timesteps, HiddenStateSize)\n",
        "            - O: The output for the current layer of shape (Batch_size, timesteps, outputsize)\n",
        "\n",
        "            Returns :\n",
        "            - dW1: The gradient for W1\n",
        "            - dW2: The gradient for W2\n",
        "            - dW3: The gradient for W3\n",
        "            - dbh: The gradient for bh\n",
        "            - dbo: The gradient for bo\n",
        "            - dh: The gradient for the hidden state at timestep t\n",
        "            - loss: The current loss \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        dW1 = np.zeros_like(self.W1)\n",
        "        dW2 = np.zeros_like(self.W2)\n",
        "        dW3 = np.zeros_like(self.W3)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dbo = np.zeros_like(self.bo)\n",
        "        dh_next = np.zeros_like(H[:, 0, :])\n",
        "        hprev = None\n",
        "\n",
        "        batchsz, T_x, inptdm = Xt.shape\n",
        "        loss = 0\n",
        "        for t in reversed(range(T_x)):\n",
        "\n",
        "            # this if-else block can be removed! and for hprev, we can simply\n",
        "            # use H[:,t -1, : ] instead, but I also add this in case it makes a\n",
        "            # a difference! so far I have not seen any difference though!\n",
        "            if t > 0:\n",
        "                hprev = H[:, t - 1, :]\n",
        "            else:\n",
        "                hprev = np.zeros_like(H[:, 0, :])\n",
        "\n",
        "            dw_1, dw_2, dw_3, db_h, db_o, dh_prev, e = self.rnn_cell_backward(Xt[:, t, :],\n",
        "                                                                              H[:, t, :],\n",
        "                                                                              hprev,\n",
        "                                                                              O[:, t, :],\n",
        "                                                                              labels[:, t, :],\n",
        "                                                                              dh_next)\n",
        "            dh_next = dh_prev\n",
        "            dW1 += dw_1\n",
        "            dW2 += dw_2\n",
        "            dW3 += dw_3\n",
        "            dbh += db_h[0]\n",
        "            dbo += db_o\n",
        "\n",
        "            # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
        "            loss -= np.log(e)\n",
        "\n",
        "        return dW1, dW2, dW3, dbh, dbo, dh_next, loss\n",
        "\n",
        "    def update_parameters(self, dW1, dW2, dW3, dbh, dbo, lr):\n",
        "        \"\"\"\n",
        "            Updates the parameters. \n",
        "            Inputs:\n",
        "            - lr : the learning rate used for updaing the parameters. \n",
        "            - dW1: The gradient for W1\n",
        "            - dW2: The gradient for W2\n",
        "            - dW3: The gradient for W3\n",
        "            - dbh: The gradient for bh\n",
        "            - dbo: The gradient for bo\n",
        "\n",
        "            Returns : \n",
        "            - W1: The updated W1\n",
        "            - W2: The updated W2\n",
        "            - W3: The updated W3\n",
        "            - bh: The updated bh\n",
        "            - bo: The updated bo\n",
        "        \"\"\"\n",
        "\n",
        "        self.W1 += -lr * dW1\n",
        "        self.W2 += -lr * dW2\n",
        "        self.W3 += -lr * dW3\n",
        "        self.bh += -lr * dbh\n",
        "        self.bo += -lr * dbo\n",
        "\n",
        "        return self.W1, self.W2, self.W3, self.bh, self.bo\n",
        "\n",
        "    def clip(self, dW1, dW2, dW3, dbh, dbo, maxValue=0.5):\n",
        "        '''\n",
        "        Clips the gradients' values between minimum and maximum.\n",
        "        Arguments:\n",
        "        The gradients \"dW1\", \"dW2\", \"dW3\", \"db\", \"dby\"\n",
        "        maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
        "        Returns: \n",
        "        gradients -- \"dW1\", \"dW2\", \"dW3\", \"db\", \"dby\"\n",
        "        '''\n",
        "        for gradient in [dW1, dW2, dW3, dbh, dbo]:\n",
        "            np.clip(gradient, a_min=-maxValue, a_max=maxValue, out=gradient)\n",
        "\n",
        "        return dW1, dW2, dW3, dbh, dbo\n",
        "\n",
        "    def optimize(self, X, Y, H, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "            Runs a complete forward-backward pass and then updates\n",
        "            The weights.\n",
        "            Inputs:\n",
        "            - X: The input data of shape (Batch_size, time_steps, input_dim_size)\n",
        "            - Y: The labels for the input data of shape (Batch_size, time_steps, outputsize)\n",
        "            - H: (Optional), The hiddenstates of shape (Batchsize, timesteps, hiddenstateSize)\n",
        "              that you can provide, so that in the forward pass, the initial hiddenstate uses\n",
        "              its last timestep hiddenstate for the previous examples forward pass. Initially \n",
        "              array must be all zeros, then after the first fowardpass, you return the resulting \n",
        "              H, and for the next round, use that H! We will see how this works when we try to \n",
        "              use our class on a test data!\n",
        "\n",
        "            Returns : \n",
        "            - loss\n",
        "            - dW1: The gradient for W1\n",
        "            - dW2: The gradient for W2\n",
        "            - dW3: The gradient for W3\n",
        "            - dbh: The gradient for bh\n",
        "            - dbo: The gradient for bo\n",
        "            - H: The hiddenstates for this round of forward pas\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Forward propagate through time\n",
        "        O, H = self.rnn_layer_forward(X, H)\n",
        "\n",
        "        # Backpropagate through time\n",
        "        dW1, dW2, dW3, dbh, dbo, dh, loss = self.rnn_layer_backward(X, Y, H, O)\n",
        "\n",
        "\n",
        "        #dW1, dW2, dW3, dbh, dbo = self.clip( dW1, dW2, dW3, dbh, dbo, maxValue=self.clipping_threshold)\n",
        "\n",
        "\n",
        "        # Update parameters\n",
        "        W1, dW2, dW3, dbh, dbo = self.update_parameters( dW1, dW2, dW3, dbh, dbo, learning_rate)\n",
        "\n",
        "        return loss, dW1, dW2, dW3, dbh, dbo, H\n",
        "\n",
        "    def gradient_checking(self, X, Y, epsilon=1e-5):\n",
        "\n",
        "        # Forward propagate through time\n",
        "        O, H = self.rnn_layer_forward(X, H=None)\n",
        "        # Backpropagate through time\n",
        "        dW1, dW2, dW3, dbh, dbo, dh, loss = self.rnn_layer_backward(X, Y, H, O)\n",
        "\n",
        "        for param, dparam, name in zip([self.W1, self.W2, self.W3, self.bh, self.bo],\n",
        "                                       [dW1,     dW2,     dW3,     dbh,      dbo],\n",
        "                                       ['W1',    'W2',    'W3',    'bh',     'bo']):\n",
        "            s0 = param.shape\n",
        "            s1 = dparam.shape\n",
        "\n",
        "            assert s0 == s1, 'Error! @ {} dimensions must match! and here {} != {} '.format(name, s0, s1)\n",
        "\n",
        "            print('{}:'.format(name))\n",
        "\n",
        "            # number of checks for each parameter\n",
        "            num_checks = 3\n",
        "\n",
        "            for i in range(num_checks):\n",
        "\n",
        "                ri = int(np.random.uniform(0, param.size))\n",
        "                old_val = param.flat[ri]\n",
        "\n",
        "                param.flat[ri] = old_val + epsilon\n",
        "\n",
        "                # Forward propagate through time\n",
        "                O, H = self.rnn_layer_forward(X, H=None)\n",
        "                # Backpropagate through time\n",
        "                dW1, dW2, dW3, dbh, dbo, dh, loss0 = self.rnn_layer_backward( X, Y, H, O)\n",
        "\n",
        "                param.flat[ri] = old_val - epsilon\n",
        "\n",
        "                # Forward propagate through time\n",
        "                O, H = self.rnn_layer_forward(X, H=None)\n",
        "                # Backpropagate through time\n",
        "                dW1, dW2, dW3, dbh, dbo, dh, loss1 = self.rnn_layer_backward( X, Y, H, O)\n",
        "\n",
        "                # restore the original value\n",
        "                param.flat[ri] = old_val\n",
        "\n",
        "                grad_analytical = dparam.flat[ri]\n",
        "                grad_numerical = (loss0 - loss1) / (2 * epsilon)\n",
        "\n",
        "                relative_error = abs(\n",
        "                    grad_analytical - grad_numerical) / abs(grad_numerical + grad_analytical)\n",
        "\n",
        "                print('{}, {} => error: {} (error should be less than {})'.format(\n",
        "                    grad_analytical, grad_numerical, relative_error, 1e-7))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}